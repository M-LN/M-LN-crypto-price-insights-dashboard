{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b8664fa",
   "metadata": {},
   "source": [
    "# Crypto Price Insights â€“ Exploratory Notebook\n",
    "\n",
    "This notebook walks through an API-driven crypto analytics workflow that powers the \"Crypto Price Insights Dashboard\" portfolio project. Follow the sections to fetch data, engineer features, surface insights, and prototype automation-ready artifacts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f19f62a8",
   "metadata": {},
   "source": [
    "## 1. Set Up Environment and API Parameters\n",
    "\n",
    "We'll configure the Python environment, declare reusable helpers, and define the API endpoints used throughout the workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d413ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import logging\n",
    "import time\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, Optional\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "from requests.adapters import HTTPAdapter\n",
    "from urllib3.util.retry import Retry\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "PROJECT_ROOT = Path.cwd().parent\n",
    "DATA_DIR = PROJECT_ROOT / \"data\"\n",
    "RAW_DIR = DATA_DIR / \"raw\"\n",
    "PROCESSED_DIR = DATA_DIR / \"processed\"\n",
    "RAW_DIR.mkdir(parents=True, exist_ok=True)\n",
    "PROCESSED_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "COINGECKO_BASE_URL = \"https://api.coingecko.com/api/v3\"\n",
    "ENDPOINTS = {\n",
    "    \"market_chart\": \"/coins/{coin_id}/market_chart\",\n",
    "}\n",
    "HEADERS = {\n",
    "    \"accept\": \"application/json\",\n",
    "    \"user-agent\": \"crypto-price-insights-notebook/1.0\",\n",
    "}\n",
    "\n",
    "\n",
    "def build_market_params(coin_id: str, vs_currency: str = \"usd\", days: int = 30, interval: Optional[str] = None) -> Dict[str, Any]:\n",
    "    params: Dict[str, Any] = {\n",
    "        \"vs_currency\": vs_currency,\n",
    "        \"days\": days,\n",
    "    }\n",
    "    if interval:\n",
    "        params[\"interval\"] = interval\n",
    "    return params\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5aaed44",
   "metadata": {},
   "source": [
    "## 2. Fetch Market Data from CoinGecko API\n",
    "\n",
    "We'll configure a resilient HTTP session, retrieve 30-day market data (price, volume, market cap), and persist raw responses for reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e7c9ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_session() -> requests.Session:\n",
    "    retry = Retry(\n",
    "        total=5,\n",
    "        read=5,\n",
    "        connect=5,\n",
    "        status=5,\n",
    "        status_forcelist=(429, 500, 502, 503, 504),\n",
    "        allowed_methods=(\"GET\",),\n",
    "        backoff_factor=1.0,\n",
    "    )\n",
    "    adapter = HTTPAdapter(max_retries=retry)\n",
    "    session = requests.Session()\n",
    "    session.headers.update(HEADERS)\n",
    "    session.mount(\"https://\", adapter)\n",
    "    session.mount(\"http://\", adapter)\n",
    "    return session\n",
    "\n",
    "\n",
    "def fetch_market_chart(coin_id: str, vs_currency: str = \"usd\", days: int = 30, interval: Optional[str] = None) -> Dict[str, Any]:\n",
    "    session = create_session()\n",
    "    params = build_market_params(coin_id, vs_currency, days, interval)\n",
    "    endpoint = ENDPOINTS[\"market_chart\"].format(coin_id=coin_id)\n",
    "    url = f\"{COINGECKO_BASE_URL}{endpoint}\"\n",
    "\n",
    "    for attempt in range(5):\n",
    "        response = session.get(url, params=params, timeout=30)\n",
    "        if response.status_code == 429:\n",
    "            wait_time = min(60, 2 ** attempt)\n",
    "            logging.warning(\"Rate limited. Sleeping for %s seconds\", wait_time)\n",
    "            time.sleep(wait_time)\n",
    "            continue\n",
    "        response.raise_for_status()\n",
    "        payload = response.json()\n",
    "        timestamp = int(time.time())\n",
    "        raw_path = RAW_DIR / f\"{coin_id}_market_chart_{timestamp}.json\"\n",
    "        raw_path.write_text(json.dumps(payload, indent=2))\n",
    "        logging.info(\"Saved raw payload to %s\", raw_path)\n",
    "        return payload\n",
    "\n",
    "    raise RuntimeError(\"Unable to fetch data after multiple attempts\")\n",
    "\n",
    "\n",
    "coin_id = \"bitcoin\"\n",
    "vs_currency = \"usd\"\n",
    "lookback_days = 30\n",
    "raw_payload = fetch_market_chart(coin_id=coin_id, vs_currency=vs_currency, days=lookback_days)\n",
    "list(raw_payload.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27fe74a4",
   "metadata": {},
   "source": [
    "## 3. Normalize and Clean Time Series Data\n",
    "\n",
    "Next we convert the raw JSON into tidy pandas DataFrames, align timestamps, forward-fill gaps, and eliminate duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c6502d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalise_market_payload(payload: Dict[str, Any]) -> pd.DataFrame:\n",
    "    def _to_frame(key: str, value_name: str) -> pd.DataFrame:\n",
    "        frame = pd.DataFrame(payload.get(key, []), columns=[\"timestamp\", value_name])\n",
    "        if frame.empty:\n",
    "            raise ValueError(f\"Payload missing expected key: {key}\")\n",
    "        frame[\"timestamp\"] = pd.to_datetime(frame[\"timestamp\"], unit=\"ms\", utc=True)\n",
    "        return frame\n",
    "\n",
    "    price_frame = _to_frame(\"prices\", \"price\")\n",
    "    cap_frame = _to_frame(\"market_caps\", \"market_cap\")\n",
    "    volume_frame = _to_frame(\"total_volumes\", \"volume\")\n",
    "\n",
    "    merged = price_frame.merge(cap_frame, on=\"timestamp\", how=\"outer\").merge(volume_frame, on=\"timestamp\", how=\"outer\")\n",
    "    merged = merged.sort_values(\"timestamp\").drop_duplicates(subset=[\"timestamp\"], keep=\"last\")\n",
    "    merged = merged.set_index(\"timestamp\")\n",
    "    merged = merged.tz_convert(\"UTC\").tz_localize(None)\n",
    "\n",
    "    inferred_freq = pd.infer_freq(merged.index[:10])\n",
    "    if inferred_freq:\n",
    "        full_range = pd.date_range(start=merged.index.min(), end=merged.index.max(), freq=inferred_freq)\n",
    "        merged = merged.reindex(full_range)\n",
    "\n",
    "    merged = merged.ffill().bfill()\n",
    "    return merged\n",
    "\n",
    "\n",
    "market_df = normalise_market_payload(raw_payload)\n",
    "market_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64173f6c",
   "metadata": {},
   "source": [
    "## 4. Engineer Financial Features\n",
    "\n",
    "We derive log returns, rolling averages, exponential moving averages, and realized volatility across configurable windows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "322e8147",
   "metadata": {},
   "outputs": [],
   "source": [
    "def engineer_features(frame: pd.DataFrame, windows: tuple[int, ...] = (7, 14, 30)) -> pd.DataFrame:\n",
    "    features = frame.copy()\n",
    "    features[\"log_return\"] = np.log(features[\"price\"]).diff()\n",
    "    features[\"simple_return\"] = features[\"price\"].pct_change()\n",
    "\n",
    "    for window in windows:\n",
    "        features[f\"rolling_mean_{window}\"] = features[\"price\"].rolling(window).mean()\n",
    "        features[f\"ema_{window}\"] = features[\"price\"].ewm(span=window, adjust=False).mean()\n",
    "        features[f\"realized_vol_{window}\"] = features[\"log_return\"].rolling(window).std() * np.sqrt(365)\n",
    "\n",
    "    features = features.dropna()\n",
    "    processed_path = PROCESSED_DIR / f\"{coin_id}_features.parquet\"\n",
    "    features.to_parquet(processed_path)\n",
    "    logging.info(\"Saved processed features to %s\", processed_path)\n",
    "    return features\n",
    "\n",
    "\n",
    "feature_df = engineer_features(market_df)\n",
    "feature_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aced3751",
   "metadata": {},
   "source": [
    "## 5. Summarize Statistical Metrics\n",
    "\n",
    "We compute descriptive statistics, volatility estimates, and downside risk metrics to support storyline-ready insights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10df06fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarise_metrics(features: pd.DataFrame) -> tuple[pd.DataFrame, dict[str, float]]:\n",
    "    returns = features[\"log_return\"]\n",
    "    mean_return = returns.mean()\n",
    "    sigma_d = np.sqrt(((returns - mean_return) ** 2).sum() / (len(returns) - 1))\n",
    "    annualised_vol = sigma_d * np.sqrt(365)\n",
    "    downside_returns = returns[returns < 0]\n",
    "    downside_sigma = np.sqrt(((downside_returns - downside_returns.mean()) ** 2).sum() / max(len(downside_returns) - 1, 1))\n",
    "    downside_risk = downside_sigma * np.sqrt(365)\n",
    "\n",
    "    summary_table = features[[\"price\", \"simple_return\", \"log_return\"]].describe().T\n",
    "    summary_metrics = {\n",
    "        \"mean_return\": mean_return,\n",
    "        \"daily_volatility\": sigma_d,\n",
    "        \"annualised_volatility\": annualised_vol,\n",
    "        \"downside_risk\": downside_risk,\n",
    "        \"max_drawdown\": features[\"price\"].div(features[\"price\"].cummax()).min() - 1,\n",
    "        \"latest_price\": features[\"price\"].iloc[-1],\n",
    "    }\n",
    "    return summary_table, summary_metrics\n",
    "\n",
    "\n",
    "summary_table, summary_metrics = summarise_metrics(feature_df)\n",
    "summary_table, summary_metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ff4c11",
   "metadata": {},
   "source": [
    "## 6. Visualize Price Dynamics\n",
    "\n",
    "We'll plot price trends with overlays, inspect return distributions, and check autocorrelation for serial dependence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd062eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use(\"seaborn-v0_8\")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 5))\n",
    "ax.plot(feature_df.index, feature_df[\"price\"], label=\"Close\", color=\"#1f77b4\")\n",
    "for window in (7, 14, 30):\n",
    "    ax.plot(feature_df.index, feature_df[f\"rolling_mean_{window}\"], label=f\"SMA {window}\", linestyle=\"--\")\n",
    "ax.set_title(f\"{coin_id.title()} price trend with rolling means\")\n",
    "ax.set_ylabel(f\"Price ({vs_currency.upper()})\")\n",
    "ax.legend()\n",
    "ax.grid(alpha=0.3)\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 4))\n",
    "feature_df[\"simple_return\"].hist(ax=ax, bins=40, density=True, alpha=0.6, label=\"Histogram\")\n",
    "feature_df[\"simple_return\"].plot(kind=\"kde\", ax=ax, color=\"black\", label=\"KDE\")\n",
    "ax.set_title(\"Daily return distribution\")\n",
    "ax.set_xlabel(\"Return\")\n",
    "ax.legend()\n",
    "plt.show()\n",
    "\n",
    "from pandas.plotting import autocorrelation_plot\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 4))\n",
    "autocorrelation_plot(feature_df[\"simple_return\"], ax=ax)\n",
    "ax.set_title(\"Autocorrelation of daily returns\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "902636a3",
   "metadata": {},
   "source": [
    "## 7. Prototype Directional Classifier\n",
    "\n",
    "We label next-day direction, split into train/test sets, fit a baseline logistic regression, and evaluate performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa737289",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_df = feature_df.copy()\n",
    "classifier_df[\"target\"] = (classifier_df[\"price\"].shift(-1) > classifier_df[\"price\"]).astype(int)\n",
    "classifier_df = classifier_df.dropna()\n",
    "\n",
    "feature_columns = [\n",
    "    \"simple_return\",\n",
    "    \"log_return\",\n",
    "    \"rolling_mean_7\",\n",
    "    \"rolling_mean_14\",\n",
    "    \"rolling_mean_30\",\n",
    "    \"ema_7\",\n",
    "    \"ema_14\",\n",
    "    \"ema_30\",\n",
    "    \"realized_vol_7\",\n",
    "    \"realized_vol_14\",\n",
    "    \"realized_vol_30\",\n",
    "]\n",
    "\n",
    "X = classifier_df[feature_columns]\n",
    "y = classifier_df[\"target\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, shuffle=False)\n",
    "model = LogisticRegression(max_iter=500)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "baseline_accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Logistic regression accuracy: {baseline_accuracy:.3f}\")\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64c432e7",
   "metadata": {},
   "source": [
    "## 8. Schedule Automated Data Refresh\n",
    "\n",
    "We'll sketch a CLI entrypoint that orchestrates fetch, transform, and reporting runs, and outline scheduling options for cron and Windows Task Scheduler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "547d54ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from textwrap import dedent\n",
    "\n",
    "cli_path = PROJECT_ROOT / \"scripts\" / \"run_pipeline.py\"\n",
    "cli_code = dedent(\n",
    "    \"\"\"\n",
    "    import argparse\n",
    "    import logging\n",
    "    import sys\n",
    "    from pathlib import Path\n",
    "\n",
    "    PROJECT_ROOT = Path(__file__).resolve().parents[1]\n",
    "    SRC_DIR = PROJECT_ROOT / \"src\"\n",
    "    if str(SRC_DIR) not in sys.path:\n",
    "        sys.path.insert(0, str(SRC_DIR))\n",
    "\n",
    "    from crypto_dashboard.pipeline import run_pipeline\n",
    "\n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO,\n",
    "        format=\"%(asctime)s [%(levelname)s] %(message)s\",\n",
    "    )\n",
    "\n",
    "    def build_parser() -> argparse.ArgumentParser:\n",
    "        parser = argparse.ArgumentParser(description=\"Run the crypto price insights pipeline\")\n",
    "        parser.add_argument(\"--coin-id\", default=\"bitcoin\", help=\"CoinGecko coin identifier\")\n",
    "        parser.add_argument(\"--vs-currency\", default=\"usd\", help=\"Quote currency\")\n",
    "        parser.add_argument(\"--days\", type=int, default=90, help=\"Number of days of history\")\n",
    "        parser.add_argument(\n",
    "            \"--export\",\n",
    "            type=Path,\n",
    "            default=PROJECT_ROOT / \"data\" / \"processed\" / \"latest_enriched.csv\",\n",
    "            help=\"CSV export path\",\n",
    "        )\n",
    "        return parser\n",
    "\n",
    "    def main() -> None:\n",
    "        args = build_parser().parse_args()\n",
    "        logging.info(\"Running pipeline for %s/%s over %s days\", args.coin_id, args.vs_currency, args.days)\n",
    "        results = run_pipeline(\n",
    "            coin_id=args.coin_id,\n",
    "            vs_currency=args.vs_currency,\n",
    "            days=args.days,\n",
    "            export_path=args.export,\n",
    "        )\n",
    "        logging.info(\"Exported enriched dataset to %s\", args.export)\n",
    "        logging.info(\"Computed metrics: %s\", results[\"metrics\"])\n",
    "\n",
    "    if __name__ == \"__main__\":\n",
    "        main()\n",
    "    \"\"\"\n",
    ")\n",
    "cli_path.write_text(cli_code)\n",
    "cli_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31034ec6",
   "metadata": {},
   "source": [
    "### Scheduling tips\n",
    "\n",
    "- **Cron (Linux/macOS)**: `0 6 * * * /usr/bin/env PYTHONPATH=\"/path/to/project/src\" /usr/bin/python /path/to/project/scripts/run_pipeline.py --coin-id bitcoin --days 90`\n",
    "- **Windows Task Scheduler**: Create a *Basic Task*, point to `powershell.exe`, and use an argument like `-Command \"Set-Location 'C:\\\\path\\\\to\\\\project'; $env:PYTHONPATH='src'; python .\\\\scripts\\\\run_pipeline.py --coin-id bitcoin --days 90\"`.\n",
    "- Capture logs by redirecting stdout/stderr to a log file for later inspection, e.g. append `>> logs/pipeline.log 2>&1`.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
